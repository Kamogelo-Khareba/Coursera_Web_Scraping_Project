{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define a Data Wrangling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangler(response):\n",
    "    # Converting the response to a dictionary\n",
    "    data_base = response.json()\n",
    "    # Converting the dictionary to a pandas Dataframe\n",
    "    df = pd.DataFrame(data_base['data'])\n",
    "    # Cleaning the company name\n",
    "    for i in range(len(df)):\n",
    "        df['company'][i] = df['company'][i]['name']\n",
    "    # Dropping columns I find not useful\n",
    "    df.drop(columns=['id','referenceId','postAt','postedTimestamp','posterId'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define a function that will write the data to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_writer(df,your_name):\n",
    "    with pd.ExcelWriter(\"Data_Storage\"+\"-\"+str(your_name)+\".xlsx\") as writer:\n",
    "        df4.to_excel(writer,sheet_name =\"Data\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Define a function collect your job title and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data using an API \n",
    "\n",
    "# Using the get request without an API mostly results in ConnectionResetError WinErr[10054]\n",
    "\n",
    "# I chose the LinkedIn API as it is the most popular website/social network for job postings\n",
    "\n",
    "def linkedIn_jobs(position:str,location_Id:str,your_name:str):\n",
    "    # LinkedIn api (Rapidapi.com)\n",
    "    url = \"https://linkedin-data-api.p.rapidapi.com/search-jobs-v2\"\n",
    "    # Querystring limited to position and location, sorting and time can be added to drill down your search\n",
    "    querystring = {\"keywords\":position,\"locationId\":location_Id}\n",
    "\n",
    "    # API Keys I got from RapidAPI.com \n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"0501448d09mshe93c5485606cd94p1f02eejsne5bd0eb0730a\",\n",
    "        \"X-RapidAPI-Host\": \"linkedin-data-api.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    \n",
    "    value_1 = data_wrangler(response)\n",
    "    \n",
    "    #### Value_2 is being commented out because the coursera jupyter notebook does not have module that supports\n",
    "    #### pd.ExcelWriter\n",
    "    \n",
    "    #value_2 = excel_writer(value_1,your_name)\n",
    "    \n",
    "    return value_1\n",
    " \n",
    "#linkedIn_jobs(\"Data Analyst\",\"104035573\",\"Kamogelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = linkedIn_jobs(\"Data Analyst\",\"104035573\",\"Kamogelo\")\n",
    "\n",
    "\n",
    "def get_job_description(query):\n",
    "    # Get all the urls from the jobs\n",
    "    links = query['url']\n",
    "    \n",
    "    # Using a for loop we scrape the urls to get the information on the page\n",
    "    # Storing the responses on inside a list \"response_store\"\n",
    "    response_store = []\n",
    "    for j in links:\n",
    "        response_store.append(requests.get(j))\n",
    "    \n",
    "    # Using BeautifulSoup to convert the response to be readable\n",
    "    # I would have loved to convert each response to a dictionary using .json()\n",
    "    # but I keep getting error. Working with dictionaries is much simpler\n",
    "    # I will use BeautifulSoup to get html like responses\n",
    "    \n",
    "    response_revealed = []\n",
    "\n",
    "    for k in response_store:\n",
    "        response_revealed.append(bs(k.content,\"html.parser\"))\n",
    "        \n",
    "    # Creating a dictionary that will store the revealed responses with its appropriate link\n",
    "    # So that we can later use the links to join the responses to the dataframe\n",
    "    \n",
    "    links_and_responses = dict(zip(links,response_revealed))\n",
    "    \n",
    "    return [response_revealed,links_and_responses]\n",
    "        \n",
    "\n",
    "html_responses = get_job_description(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes to be added\n",
    "\n",
    "#### Function to extract the Job description from the bs4 elements captured(responses_revealed)\n",
    "#### Function to link(using url as you joining key) the extracted job description to the main dataframe that will be written out to a excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
